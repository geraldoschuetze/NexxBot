import streamlit as st
import os
from langchain.chat_models import ChatOpenAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pytube import YouTube

# Ler vari√°veis do ambiente
openai_api_key = os.getenv("OPENAI_API_KEY")

# Verifica√ß√£o da chave
if not openai_api_key:
    st.error("üö® OPENAI_API_KEY n√£o est√° configurada!")
    st.stop()

# Configura√ß√£o inicial
st.set_page_config(page_title="YouTube AI Q&A", layout="wide")
st.title("üé• YouTube AI - Pergunte sobre o v√≠deo")

# Entradas do usu√°rio
video_url = st.text_input("üîó Cole a URL do v√≠deo do YouTube:")
user_question = st.text_input("‚ùì Fa√ßa uma pergunta sobre o v√≠deo:")

def get_video_transcript(video_url):
    try:
        yt = YouTube(video_url)
        caption = yt.captions.get_by_language_code("pt") or yt.captions.get_by_language_code("en")
        if not caption:
            return None
        return caption.generate_srt_captions()
    except Exception as e:
        st.error(f"Erro ao baixar a legenda: {e}")
        return None

if st.button("üîç Analisar v√≠deo"):
    if not video_url or not user_question:
        st.warning("Preencha a URL do v√≠deo e a pergunta.")
    else:
        with st.spinner("üé¨ Extraindo legendas..."):
            transcript = get_video_transcript(video_url)
            if not transcript:
                st.error("‚ö†Ô∏è N√£o foi poss√≠vel obter as legendas desse v√≠deo.")
                st.stop()

        with st.spinner("ü§ñ Processando com IA..."):
            llm = ChatOpenAI(
                temperature=0.2,
                model_name="gpt-4",
                openai_api_key=openai_api_key,
            )

            # Dividir legenda em trechos
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=100
            )
            docs = text_splitter.create_documents([transcript])

            # Carregar cadeia de QA
            qa_chain = load_qa_chain(llm, chain_type="stuff")
            response = qa_chain.run(input_documents=docs, question=user_question)

        st.success("‚úÖ Resposta gerada!")
        st.markdown(f"**Pergunta:** {user_question}")
        st.markdown(f"**Resposta:** {response}")
